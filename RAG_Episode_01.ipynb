{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LangChain version: 1.2.10\n",
      "✅ All RAG packages installed!\n",
      "\n",
      "Your toolkit:\n",
      "   - LangChain + Groq     --> LLM\n",
      "   - HuggingFace          --> Embeddings (local)\n",
      "   - FAISS                --> Vector store (local)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": "# Install the RAG toolkit\n!pip install -qU langchain langchain-core langchain-groq langchain-huggingface langchain-community faiss-cpu sentence-transformers python-dotenv\n\n# Verify installation\nimport langchain\nprint(f\"✅ LangChain version: {langchain.__version__}\")\nprint(\"✅ All RAG packages installed!\")\nprint()\nprint(\"Your toolkit:\")\nprint(\"   - LangChain + Groq     --> LLM\")\nprint(\"   - HuggingFace          --> Embeddings (local)\")\nprint(\"   - FAISS                --> Vector store (local)\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Groq API key loaded: gsk_Q08MI36d...\n",
      "AI: RAG pipeline ready\n",
      "\n",
      "LLM ready. Let's build RAG.\n"
     ]
    }
   ],
   "source": "import os\nfrom dotenv import load_dotenv\nfrom langchain_groq import ChatGroq\n\nload_dotenv()\ngroq_api_key = os.getenv(\"GROQ_API_KEY\")\n\nif not groq_api_key:\n    print(\"❌ API key not found! Create .env with: GROQ_API_KEY=your_key\")\nelse:\n    print(f\"✅ Groq API key loaded: {groq_api_key[:12]}...\")\n\n# Initialize LLM — temperature 0 for factual RAG answers\nllm = ChatGroq(\n    model=\"llama-3.3-70b-versatile\",\n    temperature=0,\n    groq_api_key=groq_api_key\n)\n\n# Quick test\nresponse = llm.invoke(\"Say 'RAG pipeline ready!' in exactly 3 words.\")\nprint(f\"AI: {response.content}\")\nprint()\nprint(\"LLM ready. Let's build RAG.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 documents\n",
      "Categories: {'Product', 'Engineering', 'Culture', 'HR'}\n",
      "Sources: {'product_info.pdf', 'culture_handbook.pdf', 'security_handbook.pdf', 'hr_policies.pdf', 'tech_docs.md'}\n",
      "\n",
      "Sample document:\n",
      "   Content: Annual leave policy: All full-time employees receive 24 days of paid annual leav...\n",
      "   Metadata: {'source': 'hr_policies.pdf', 'category': 'HR'}\n"
     ]
    }
   ],
   "source": "from langchain_core.documents import Document\n\n# Simulate a company knowledge base\ndocuments = [\n    # HR Policy\n    Document(\n        page_content=\"Annual leave policy: All full-time employees receive 24 days of paid annual leave per year. Leave accrues at 2 days per month. Unused leave up to 10 days can be carried forward to the next year. Leave beyond 10 days expires on March 31st. Employees must submit leave requests at least 3 business days in advance for planned leave. Emergency leave can be applied retroactively within 2 business days. Work from home policy: Employees can work from home up to 3 days per week. Monday and Thursday are mandatory in-office days for team collaboration. New employees in their first 90 days must work from office full-time. Reimbursement policy: Business travel expenses are reimbursed within 15 business days. Hotels are capped at Rs 5000 per night for metro cities. Meal allowance is Rs 1500 per day during travel.\",\n        metadata={\"source\": \"hr_policies.pdf\", \"category\": \"HR\"}\n    ),\n    # Tech Documentation\n    Document(\n        page_content=\"Tech stack overview: Our backend is built with Python 3.11 and FastAPI. We use PostgreSQL 15 as our primary database with Redis for caching. The frontend is React 18 with TypeScript. Deployment is through Docker containers on AWS ECS with auto-scaling. CI/CD pipelines run on GitHub Actions. We follow a trunk-based development model with feature flags for gradual rollouts. Code review guidelines: All pull requests require at least 2 approvals before merging. Reviews should be completed within 24 hours. No PR should exceed 400 lines of changed code. All tests must pass in CI before merging.\",\n        metadata={\"source\": \"tech_docs.md\", \"category\": \"Engineering\"}\n    ),\n    # Security\n    Document(\n        page_content=\"Security protocols: All API keys and secrets must be stored in AWS Secrets Manager — never in code or environment files in production. Two-factor authentication is mandatory for all company accounts. Passwords must be at least 16 characters. We conduct quarterly security audits and annual penetration testing. Any suspected security breach must be reported to security@company.com within 1 hour.\",\n        metadata={\"source\": \"security_handbook.pdf\", \"category\": \"Engineering\"}\n    ),\n    # Product Information\n    Document(\n        page_content=\"Product pricing tiers: The Starter plan costs Rs 999 per month and includes 5 users, 10GB storage, and email support. The Professional plan costs Rs 2999 per month with 25 users, 100GB storage, priority support, and API access. The Enterprise plan is custom-priced and includes unlimited users, unlimited storage, dedicated account manager, SSO integration, and 99.9% SLA guarantee. Refund policy: Customers can request a full refund within 14 days of purchase. After 14 days, a prorated refund is available for annual subscriptions only.\",\n        metadata={\"source\": \"product_info.pdf\", \"category\": \"Product\"}\n    ),\n    # Company Culture\n    Document(\n        page_content=\"Company values: We operate on five core values. First, Customer Obsession — every decision starts with the customer. Second, Bias for Action — we prefer moving fast and iterating. Third, Radical Transparency — we share company financials and challenges with all employees quarterly. Fourth, Ownership Mentality — everyone acts like a founder. Fifth, Continuous Learning — every employee gets Rs 50000 per year for courses, books, and conferences. Team rituals: Monday kickoff at 10 AM, Wednesday lunch-and-learn, Friday demos at 4 PM. Monthly town halls with the CEO. Annual hackathon in December.\",\n        metadata={\"source\": \"culture_handbook.pdf\", \"category\": \"Culture\"}\n    ),\n]\n\nprint(f\"Loaded {len(documents)} documents\")\nprint(f\"Categories: {set(d.metadata['category'] for d in documents)}\")\nprint(f\"Sources: {set(d.metadata['source'] for d in documents)}\")\nprint()\nprint(\"Sample document:\")\nprint(f\"   Content: {documents[0].page_content[:80]}...\")\nprint(f\"   Metadata: {documents[0].metadata}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original documents: 5\n",
      "After splitting:    9 chunks\n",
      "Avg chunk size:     326 characters\n",
      "\n",
      "============================================================\n",
      "\n",
      "Sample chunks (first 3):\n",
      "\n",
      "--- Chunk 1 [hr_policies.pdf] ---\n",
      "Annual leave policy: All full-time employees receive 24 days of paid annual leave per year. Leave accrues at 2 days per month. Unused leave up to 10 d...\n",
      "Length: 473 chars\n",
      "\n",
      "--- Chunk 2 [hr_policies.pdf] ---\n",
      ". Monday and Thursday are mandatory in-office days for team collaboration. New employees in their first 90 days must work from office full-time. Reimb...\n",
      "Length: 337 chars\n",
      "\n",
      "--- Chunk 3 [tech_docs.md] ---\n",
      "Tech stack overview: Our backend is built with Python 3.11 and FastAPI. We use PostgreSQL 15 as our primary database with Redis for caching. The front...\n",
      "Length: 460 chars\n"
     ]
    }
   ],
   "source": "from langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=50,\n    length_function=len,\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n)\n\nchunks = text_splitter.split_documents(documents)\n\nprint(f\"Original documents: {len(documents)}\")\nprint(f\"After splitting:    {len(chunks)} chunks\")\nprint(f\"Avg chunk size:     {sum(len(c.page_content) for c in chunks) // len(chunks)} characters\")\nprint()\nprint(\"=\" * 60)\nprint()\nprint(\"Sample chunks (first 3):\")\nfor i, chunk in enumerate(chunks[:3]):\n    print(f\"\\n--- Chunk {i+1} [{chunk.metadata['source']}] ---\")\n    print(f\"{chunk.page_content[:150]}...\")\n    print(f\"Length: {len(chunk.page_content)} chars\")"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding model loaded (runs locally!)\n",
      "Embedding dimensions: 384\n",
      "First 10 values: [0.0538, 0.0522, 0.0, 0.036, 0.1113, 0.117, 0.037, -0.1007, -0.0746, 0.006]\n",
      "\n",
      "============================================================\n",
      "\n",
      "Semantic Similarity Demo:\n",
      "\n",
      "   'How many vacation days do I get?'\n",
      "   vs 'What is the annual leave policy?'\n",
      "   Similarity: 0.4222  <-- HIGH (same topic!)\n",
      "\n",
      "   'How many vacation days do I get?'\n",
      "   vs 'How do I deploy a Docker container?'\n",
      "   Similarity: 0.0215  <-- LOW (different topics)\n",
      "\n",
      "✅ Embeddings capture MEANING, not just keywords!\n"
     ]
    }
   ],
   "source": "from langchain_huggingface import HuggingFaceEmbeddings\nimport numpy as np\n\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n    model_kwargs={\"device\": \"cpu\"}\n)\n\nprint(\"✅ Embedding model loaded (runs locally!)\")\n\nsample = embeddings.embed_query(\"What is the leave policy?\")\nprint(f\"Embedding dimensions: {len(sample)}\")\nprint(f\"First 10 values: {[round(x, 4) for x in sample[:10]]}\")\n\nprint()\nprint(\"=\" * 60)\nprint()\nprint(\"Semantic Similarity Demo:\")\n\nsentences = [\n    \"How many vacation days do I get?\",\n    \"What is the annual leave policy?\",\n    \"How do I deploy a Docker container?\",\n]\n\nvecs = [embeddings.embed_query(s) for s in sentences]\n\nfrom numpy import dot\nfrom numpy.linalg import norm\n\ndef cosine_sim(a, b):\n    return dot(a, b) / (norm(a) * norm(b))\n\nprint(f\"\\n   '{sentences[0]}'\")\nprint(f\"   vs '{sentences[1]}'\")\nprint(f\"   Similarity: {cosine_sim(vecs[0], vecs[1]):.4f}  <-- HIGH (same topic!)\")\n\nprint(f\"\\n   '{sentences[0]}'\")\nprint(f\"   vs '{sentences[2]}'\")\nprint(f\"   Similarity: {cosine_sim(vecs[0], vecs[2]):.4f}  <-- LOW (different topics)\")\n\nprint()\nprint(\"✅ Embeddings capture MEANING, not just keywords!\")"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store built with 9 chunks\n",
      "\n",
      "============================================================\n",
      "\n",
      "Testing similarity search:\n",
      "\n",
      "Query: 'How many vacation days do I get per year?'\n",
      "\n",
      "Result 1 [Source: hr_policies.pdf]:\n",
      "   Annual leave policy: All full-time employees receive 24 days of paid annual leave per year. Leave accrues at 2 days per month. Unused leave up to 10 days can be carried forward to the next year. Leave...\n",
      "\n",
      "Result 2 [Source: hr_policies.pdf]:\n",
      "   . Monday and Thursday are mandatory in-office days for team collaboration. New employees in their first 90 days must work from office full-time. Reimbursement policy: Business travel expenses are reim...\n",
      "\n",
      "✅ Found the leave policy — even though we said 'vacation' not 'leave'!\n",
      "   That's semantic search. Meaning, not keywords.\n"
     ]
    }
   ],
   "source": "from langchain_community.vectorstores import FAISS\n\nvectorstore = FAISS.from_documents(chunks, embeddings)\n\nprint(f\"✅ Vector store built with {len(chunks)} chunks\")\nprint()\nprint(\"=\" * 60)\nprint()\nprint(\"Testing similarity search:\")\n\nquery = \"How many vacation days do I get per year?\"\nprint(f\"\\nQuery: '{query}'\")\n\nresults = vectorstore.similarity_search(query, k=2)\n\nfor i, doc in enumerate(results):\n    print(f\"\\nResult {i+1} [Source: {doc.metadata['source']}]:\")\n    print(f\"   {doc.page_content[:200]}...\")\n\nprint()\nprint(\"✅ Found the leave policy — even though we said 'vacation' not 'leave'!\")\nprint(\"   That's semantic search. Meaning, not keywords.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG chain built!\n",
      "\n",
      "Pipeline: Question --> Retrieve --> Prompt --> LLM --> Answer\n",
      "\n",
      "Ready to answer questions from our knowledge base.\n"
     ]
    }
   ],
   "source": "from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n\ndef format_docs(docs):\n    formatted = []\n    for doc in docs:\n        source = doc.metadata.get('source', 'unknown')\n        formatted.append(f\"[Source: {source}]\\n{doc.page_content}\")\n    return \"\\n\\n\".join(formatted)\n\nrag_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"You are a helpful company assistant. Answer questions using ONLY the provided context.\n\nRules:\n- Answer based ONLY on the context below\n- If the answer is not in the context, say: \"I don't have that information in my knowledge base.\"\n- Always cite the source document\n- Be concise and specific\"\"\"),\n    (\"human\", \"\"\"Context:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\")\n])\n\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | rag_prompt\n    | llm\n    | StrOutputParser()\n)\n\nprint(\"✅ RAG chain built!\")\nprint()\nprint(\"Pipeline: Question --> Retrieve --> Prompt --> LLM --> Answer\")\nprint()\nprint(\"Ready to answer questions from our knowledge base.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How many days of annual leave do employees get?\n",
      "A: According to the hr_policies.pdf, full-time employees receive 24 days of paid annual leave per year.\n",
      "------------------------------------------------------------\n",
      "Q: Which days are mandatory to come to office?\n",
      "A: Monday and Thursday are mandatory in-office days for team collaboration. [Source: hr_policies.pdf]\n",
      "------------------------------------------------------------\n",
      "Q: What's included in the Professional pricing plan?\n",
      "A: The Professional plan includes 25 users, 100GB storage, priority support, and API access. [Source: product_info.pdf]\n",
      "------------------------------------------------------------\n",
      "Q: When is the CEO's birthday?\n",
      "A: I don't have that information in my knowledge base. [Source: culture_handbook.pdf, product_info.pdf]\n",
      "------------------------------------------------------------\n",
      "Q: How much learning budget does each employee get?\n",
      "A: Each employee gets Rs 50,000 per year for courses, books, and conferences [Source: culture_handbook.pdf].\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": "test_questions = [\n    \"How many days of annual leave do employees get?\",\n    \"Which days are mandatory to come to office?\",\n    \"What's included in the Professional pricing plan?\",\n    \"When is the CEO's birthday?\",\n    \"How much learning budget does each employee get?\",\n]\n\nfor q in test_questions:\n    print(f\"Q: {q}\")\n    answer = rag_chain.invoke(q)\n    print(f\"A: {answer}\")\n    print(\"-\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the code review process and how many approvals are needed?\n",
      "\n",
      "Answer:\n",
      "According to [Source: tech_docs.md], all pull requests require at least 2 approvals before merging. Reviews should be completed within 24 hours.\n",
      "\n",
      "Sources used:\n",
      "   - product_info.pdf (Product)\n",
      "   - tech_docs.md (Engineering)\n",
      "\n",
      "============================================================\n",
      "\n",
      "Q: What happens to my unused leave days?\n",
      "\n",
      "Answer:\n",
      "According to the hr_policies.pdf, unused leave up to 10 days can be carried forward to the next year, and leave beyond 10 days expires on March 31st. [Source: hr_policies.pdf]\n",
      "\n",
      "Sources used:\n",
      "   - product_info.pdf (Product)\n",
      "   - hr_policies.pdf (HR)\n"
     ]
    }
   ],
   "source": "def rag_with_sources(question):\n    docs = retriever.invoke(question)\n    context = format_docs(docs)\n    prompt_messages = rag_prompt.format_messages(context=context, question=question)\n    answer = llm.invoke(prompt_messages).content\n    sources = list(set(f\"{d.metadata['source']} ({d.metadata['category']})\" for d in docs))\n    return answer, sources, docs\n\nquestion = \"What is the code review process and how many approvals are needed?\"\nanswer, sources, docs = rag_with_sources(question)\n\nprint(f\"Q: {question}\")\nprint(f\"\\nAnswer:\\n{answer}\")\nprint(f\"\\nSources used:\")\nfor s in sources:\n    print(f\"   - {s}\")\n\nprint()\nprint(\"=\" * 60)\n\nquestion2 = \"What happens to my unused leave days?\"\nanswer2, sources2, _ = rag_with_sources(question2)\nprint(f\"\\nQ: {question2}\")\nprint(f\"\\nAnswer:\\n{answer2}\")\nprint(f\"\\nSources used:\")\nfor s in sources2:\n    print(f\"   - {s}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting interactive RAG demo...\n",
      "\n",
      "============================================================\n",
      "COMPANY KNOWLEDGE BASE — RAG CHATBOT\n",
      "============================================================\n",
      "\n",
      "Ask anything about company policies, tech, product, or culture.\n",
      "Type 'quit' to exit.\n",
      "\n",
      "Answer: According to the hr_policies.pdf, the annual leave policy is as follows: \n",
      "- All full-time employees receive 24 days of paid annual leave per year.\n",
      "- Leave accrues at 2 days per month.\n",
      "- Unused leave up to 10 days can be carried forward to the next year.\n",
      "- Leave beyond 10 days expires on March 31st.\n",
      "- Employees must submit leave requests at least 3 business days in advance for planned leave.\n",
      "- Emergency leave can be applied retroactively within 2 business days. [Source: hr_policies.pdf]\n",
      "\n",
      "Sources: hr_policies.pdf (HR), culture_handbook.pdf (Culture)\n",
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": "def interactive_rag():\n    print()\n    print(\"=\" * 60)\n    print(\"COMPANY KNOWLEDGE BASE — RAG CHATBOT\")\n    print(\"=\" * 60)\n    print()\n    print(\"Ask anything about company policies, tech, product, or culture.\")\n    print(\"Type 'quit' to exit.\")\n\n    while True:\n        user_input = input(\"\\nYou: \").strip()\n\n        if user_input.lower() in ['quit', 'exit', 'bye']:\n            print(\"\\nGoodbye!\")\n            break\n\n        if not user_input:\n            continue\n\n        try:\n            answer, sources, _ = rag_with_sources(user_input)\n            print(f\"\\nAnswer: {answer}\")\n            print(f\"\\nSources: {', '.join(sources)}\")\n        except Exception as e:\n            print(f\"\\nError: {str(e)}\")\n            break\n\nprint(\"Starting interactive RAG demo...\")\ninteractive_rag()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_videos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}