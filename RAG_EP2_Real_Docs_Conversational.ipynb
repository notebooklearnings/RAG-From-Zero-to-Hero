{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[title]\n",
    "RAG Episode 2: Chat With Real PDFs and Remember Every Question\n",
    "\n",
    "[narration]\n",
    "In the last episode, you built a RAG system from scratch. You understood the five step pipeline. Load, Split, Embed, Store, Retrieve. You asked questions about a company knowledge base and got accurate answers with sources.\n",
    "\n",
    "But it had two big limitations.\n",
    "\n",
    "First, we used hardcoded text. In the real world, you have PDFs, text files, web pages, Word documents. You need to load real files.\n",
    "\n",
    "Second, our system had no memory. Every question was independent. You couldn't ask a follow-up like, 'Tell me more about that' or 'What did you just say about the pricing?' Real chatbots remember the conversation.\n",
    "\n",
    "Today, we fix both.\n",
    "\n",
    "By the end of this episode, you'll have a RAG system that loads real PDF and text files, remembers your entire conversation, handles follow-up questions naturally, and shows you how relevant each result is with similarity scores.\n",
    "\n",
    "Same stack. Groq for the language model. HuggingFace for embeddings. FAISS — pronounced 'face' — for vector search. All free.\n",
    "\n",
    "Let's level up.\n",
    "\n",
    "[display]\n",
    "## What's New in Episode 2\n",
    "\n",
    "**Upgrading from Episode 1:**\n",
    "```\n",
    "Ep 1: Hardcoded text     --> Ep 2: Real PDF & text files\n",
    "Ep 1: No memory          --> Ep 2: Conversational RAG\n",
    "Ep 1: Basic retrieval    --> Ep 2: Scored similarity search\n",
    "Ep 1: Single questions   --> Ep 2: Follow-up questions work\n",
    "```\n",
    "\n",
    "**Tech Stack (same, all free):**\n",
    "```\n",
    "LangChain           --> Framework\n",
    "Groq (Llama 3.3)    --> LLM (free)\n",
    "HuggingFace         --> Embeddings (free, local)\n",
    "FAISS               --> Vector store (free, local)\n",
    "```\n",
    "\n",
    "**Prerequisites:** RAG Episode 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[title]\n",
    "Setup — Getting Everything Ready\n",
    "\n",
    "[narration]\n",
    "Quick setup. Same packages as Episode 1, plus py-PDF for reading PDF files. If you've already installed everything from the last episode, this will finish in seconds.\n",
    "\n",
    "Then we load our Groq API key and initialize the language model and embedding model. Same pattern you already know.\n",
    "\n",
    "[display]\n",
    "## Setup\n",
    "\n",
    "**New package:**\n",
    "```\n",
    "pypdf --> Read PDF files\n",
    "```\n",
    "\n",
    "**Everything else — same as Episode 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Groq API key loaded: gsk_Q08MI36d...\n",
      "\n",
      "✅ LLM ready (Groq — Llama 3.3)\n",
      "✅ Embeddings ready (HuggingFace — local)\n",
      "\n",
      "Let's load some real documents!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"[narration]\n",
    "Same install as Episode 1, plus py-PDF for PDF loading. The quiet and upgrade flags keep things clean.\n",
    "\n",
    "Then we load our API key and initialize both models. Language model at temperature zero for factual answers. Embedding model running locally on your CPU.\n",
    "\"\"\"\n",
    "\n",
    "!pip install -qU langchain langchain-core langchain-groq langchain-huggingface langchain-community faiss-cpu sentence-transformers python-dotenv pypdf\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not groq_api_key:\n",
    "    print(\"❌ API key not found! Create .env with: GROQ_API_KEY=your_key\")\n",
    "else:\n",
    "    print(f\"✅ Groq API key loaded: {groq_api_key[:12]}...\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0,\n",
    "    groq_api_key=groq_api_key\n",
    ")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"}\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"✅ LLM ready (Groq — Llama 3.3)\")\n",
    "print(\"✅ Embeddings ready (HuggingFace — local)\")\n",
    "print()\n",
    "print(\"Let's load some real documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[title]\n",
    "Creating Real Documents to Work With\n",
    "\n",
    "[narration]\n",
    "Before we load files, we need files to load. I'm going to create realistic documents right here in the notebook — a PDF and multiple text files — so you can see the entire flow without needing to download anything.\n",
    "\n",
    "In your real projects, you'd skip this step. You already have the PDFs and documents. But for learning, this keeps everything self-contained and reproducible.\n",
    "\n",
    "I'm creating a startup's documentation. An employee handbook as a PDF. A technical architecture document as a text file. And a product roadmap as another text file. Three files, three different formats, realistic content.\n",
    "\n",
    "[display]\n",
    "## Creating Sample Documents\n",
    "\n",
    "**Why create them in the notebook?**\n",
    "- Self-contained — no downloads needed\n",
    "- Reproducible — anyone can run this\n",
    "- You see the exact content being loaded\n",
    "\n",
    "**Documents we'll create:**\n",
    "```\n",
    "employee_handbook.pdf     --> HR policies, benefits\n",
    "tech_architecture.txt     --> System design, stack\n",
    "product_roadmap.txt       --> Features, timeline\n",
    "```\n",
    "\n",
    "**In YOUR projects:** Skip this step — use your own files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created: company_docs/employee_handbook.pdf (2 pages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\"\"\"[narration]\n",
    "First, I'm creating a PDF using a lightweight library called F-PDF. This generates a real PDF file that our loader will read just like any PDF you'd get from HR or legal.\n",
    "\n",
    "The employee handbook has sections on leave policy, remote work, compensation, and professional development. Each section is detailed enough that our RAG system has real content to search through.\n",
    "\"\"\"\n",
    "\n",
    "!pip install -q fpdf\n",
    "\n",
    "from fpdf import FPDF\n",
    "import os\n",
    "\n",
    "os.makedirs(\"company_docs\", exist_ok=True)\n",
    "\n",
    "pdf = FPDF()\n",
    "pdf.set_auto_page_break(auto=True, margin=15)\n",
    "\n",
    "# Page 1 - Leave & Remote Work\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", \"B\", 16)\n",
    "pdf.cell(0, 10, \"NovaTech Employee Handbook 2026\", ln=True, align=\"C\")\n",
    "pdf.ln(10)\n",
    "\n",
    "pdf.set_font(\"Arial\", \"B\", 13)\n",
    "pdf.cell(0, 8, \"Section 1: Leave Policy\", ln=True)\n",
    "pdf.set_font(\"Arial\", \"\", 11)\n",
    "pdf.multi_cell(0, 6, \"\"\"All full-time employees at NovaTech receive 28 days of paid time off per year, which includes 20 days of annual leave, 5 days of sick leave, and 3 days of personal leave. Annual leave accrues monthly at 1.67 days per month. Unused annual leave up to 5 days may be carried forward to the next calendar year, but must be used by March 31st or it will expire. Employees must submit leave requests at least 5 business days in advance for planned leave. Emergency or sick leave can be reported within 24 hours retroactively. Leave requests are submitted through the NovaTech HR Portal and require manager approval. During peak business periods (March and September), leave requests may be subject to additional review.\"\"\")\n",
    "\n",
    "pdf.ln(5)\n",
    "pdf.set_font(\"Arial\", \"B\", 13)\n",
    "pdf.cell(0, 8, \"Section 2: Remote Work Policy\", ln=True)\n",
    "pdf.set_font(\"Arial\", \"\", 11)\n",
    "pdf.multi_cell(0, 6, \"\"\"NovaTech operates a hybrid work model. Employees may work remotely up to 3 days per week. Tuesday and Thursday are designated as in-office collaboration days for all teams. Remote work requests must be logged in the HR Portal by 9 AM on the remote day. Employees in their first 60 days of employment must work from the office full-time to complete onboarding. Managers may require in-office attendance for critical project milestones with at least 48 hours notice. NovaTech provides a one-time home office setup allowance of Rs 25000 for all eligible employees. Internet reimbursement of Rs 1000 per month is provided for employees who work remotely at least 2 days per week.\"\"\")\n",
    "\n",
    "# Page 2 - Compensation & Learning\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", \"B\", 13)\n",
    "pdf.cell(0, 8, \"Section 3: Compensation & Benefits\", ln=True)\n",
    "pdf.set_font(\"Arial\", \"\", 11)\n",
    "pdf.multi_cell(0, 6, \"\"\"Salary reviews are conducted annually in April. Performance bonuses are paid in June based on the previous fiscal year performance. The bonus pool ranges from 10% to 25% of annual base salary depending on individual and company performance. NovaTech offers comprehensive health insurance covering employees and up to 2 dependents. The insurance plan includes medical, dental, and vision coverage with a Rs 500 deductible. A company-matched retirement fund contribution of 12% of base salary is provided. Employees become fully vested after 3 years of continuous employment. Stock options are available for employees at the Senior Engineer level and above, vesting over 4 years with a 1-year cliff.\"\"\")\n",
    "\n",
    "pdf.ln(5)\n",
    "pdf.set_font(\"Arial\", \"B\", 13)\n",
    "pdf.cell(0, 8, \"Section 4: Professional Development\", ln=True)\n",
    "pdf.set_font(\"Arial\", \"\", 11)\n",
    "pdf.multi_cell(0, 6, \"\"\"NovaTech invests heavily in employee growth. Each employee receives an annual learning budget of Rs 75000 for courses, certifications, conferences, and books. Unused learning budget does not carry over. Additionally, employees can dedicate every Friday afternoon (2 PM onwards) to self-directed learning projects. The company sponsors one major tech conference per year for each team. Internal knowledge sharing sessions are held every Wednesday from 12 PM to 1 PM. Employees pursuing relevant masters degrees or professional certifications receive a 50% tuition reimbursement up to Rs 200000 per year.\"\"\")\n",
    "\n",
    "pdf.output(\"company_docs/employee_handbook.pdf\")\n",
    "print(\"✅ Created: company_docs/employee_handbook.pdf (2 pages)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created: company_docs/tech_architecture.txt\n",
      "✅ Created: company_docs/product_roadmap.txt\n",
      "\n",
      "Documents created:\n",
      "   employee_handbook.pdf (3,168 bytes)\n",
      "   product_roadmap.txt (2,187 bytes)\n",
      "   tech_architecture.txt (2,755 bytes)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"[narration]\n",
    "Now two text files. The technical architecture document describes the system design, databases, deployment strategy, and monitoring setup. The product roadmap covers features planned for the next three quarters.\n",
    "\n",
    "These are the kinds of documents that engineers and product managers actually work with every day.\n",
    "\"\"\"\n",
    "\n",
    "tech_doc = \"\"\"NovaTech Technical Architecture Document\n",
    "Last Updated: January 2026\n",
    "\n",
    "1. SYSTEM OVERVIEW\n",
    "NovaTech's platform is a microservices-based SaaS application serving over 50000 active users. The system handles approximately 2 million API requests per day with a 99.95% uptime SLA. Average response time is under 200 milliseconds for core APIs.\n",
    "\n",
    "2. BACKEND ARCHITECTURE\n",
    "The backend consists of 12 microservices built with Python 3.12 and FastAPI. Each service is independently deployable and communicates via gRPC for internal calls and REST for external APIs. The authentication service uses OAuth 2.0 with JWT tokens. Rate limiting is enforced at 1000 requests per minute per API key for standard tier and 5000 for premium tier.\n",
    "\n",
    "3. DATABASE LAYER\n",
    "Primary database is PostgreSQL 16 running on AWS RDS with read replicas in two availability zones. Redis 7.2 is used for caching with a 15-minute TTL for most queries. MongoDB is used specifically for the activity logging service due to its flexible schema. Database migrations are managed through Alembic and are required to be backwards-compatible for zero-downtime deployments.\n",
    "\n",
    "4. FRONTEND\n",
    "The frontend is a single-page application built with React 18 and TypeScript. State management uses Zustand instead of Redux for simplicity. The design system is built on top of Radix UI primitives with custom Tailwind CSS styling. The frontend is served via CloudFront CDN with edge locations in 8 countries.\n",
    "\n",
    "5. DEPLOYMENT & INFRASTRUCTURE\n",
    "All services are containerized with Docker and orchestrated via Kubernetes on AWS EKS. The CI/CD pipeline runs on GitHub Actions with the following stages: lint, unit test, integration test, security scan, build, deploy to staging, automated E2E tests, and production deploy with canary rollout. Production deployments happen every Tuesday and Thursday. Hotfixes follow an expedited pipeline with senior engineer approval. Infrastructure is managed as code using Terraform.\n",
    "\n",
    "6. MONITORING & OBSERVABILITY\n",
    "Application metrics are collected via Prometheus and visualized in Grafana dashboards. Distributed tracing uses OpenTelemetry with Jaeger as the backend. Log aggregation is handled by the ELK stack (Elasticsearch, Logstash, Kibana). PagerDuty handles alerting with a tiered escalation policy: P1 issues page the on-call engineer immediately, P2 issues within 15 minutes, P3 issues create Jira tickets for next sprint.\n",
    "\n",
    "7. SECURITY\n",
    "All data is encrypted at rest using AES-256 and in transit using TLS 1.3. API keys are rotated every 90 days. Penetration testing is conducted quarterly by an external firm. SOC 2 Type II compliance is maintained and audited annually. All code changes require security review for services handling PII or payment data.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"company_docs/tech_architecture.txt\", \"w\") as f:\n",
    "    f.write(tech_doc)\n",
    "print(\"✅ Created: company_docs/tech_architecture.txt\")\n",
    "\n",
    "roadmap_doc = \"\"\"NovaTech Product Roadmap 2026\n",
    "Status: Approved by Product Council\n",
    "\n",
    "Q1 2026 (January - March): Foundation Phase\n",
    "- AI-powered search: Replace keyword search with semantic vector search across all platform content. Expected to improve search relevance by 40%. Uses the same embedding technology as our RAG features. Budget: Rs 1500000. Status: In Progress.\n",
    "- Dashboard redesign: Complete overhaul of the analytics dashboard with real-time data streaming. New charts powered by D3.js. Custom date range filters and export to PDF. Budget: Rs 800000. Status: In Progress.\n",
    "- Mobile app v2.0: Native iOS and Android apps replacing the current React Native hybrid. Push notifications for critical alerts. Offline mode for viewing cached reports. Budget: Rs 2000000. Status: Planning.\n",
    "\n",
    "Q2 2026 (April - June): Growth Phase\n",
    "- Team collaboration features: Real-time document editing, in-app commenting, task assignments, and team workspaces. Integrates with Slack and Microsoft Teams. Budget: Rs 1200000. Status: Design Phase.\n",
    "- Enterprise SSO: SAML 2.0 and OpenID Connect support for enterprise customers. Includes directory sync with Active Directory and Okta. Required for 3 pending enterprise deals worth Rs 5000000 combined. Budget: Rs 600000. Status: Planning.\n",
    "- Advanced analytics: Predictive analytics module using machine learning. Churn prediction, usage forecasting, and anomaly detection. Requires hiring 2 ML engineers. Budget: Rs 1800000. Status: Research.\n",
    "\n",
    "Q3 2026 (July - September): Scale Phase\n",
    "- Multi-region deployment: Expand infrastructure to EU (Frankfurt) and APAC (Singapore) regions. Required for GDPR compliance with European customers. Estimated infrastructure cost increase of 35%. Budget: Rs 2500000. Status: Planning.\n",
    "- API marketplace: Allow third-party developers to build and sell integrations on the NovaTech platform. Revenue share model: 70% developer, 30% NovaTech. SDK support for Python, JavaScript, and Go. Budget: Rs 1000000. Status: Concept.\n",
    "- White-label solution: Enable enterprise customers to rebrand the platform with their own logo, colors, and domain. Tiered pricing based on customization level. Budget: Rs 900000. Status: Concept.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"company_docs/product_roadmap.txt\", \"w\") as f:\n",
    "    f.write(roadmap_doc)\n",
    "print(\"✅ Created: company_docs/product_roadmap.txt\")\n",
    "\n",
    "print()\n",
    "print(\"Documents created:\")\n",
    "for f in os.listdir(\"company_docs\"):\n",
    "    size = os.path.getsize(f\"company_docs/{f}\")\n",
    "    print(f\"   {f} ({size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[title]\n",
    "Loading Real Files — PDF Loader and Text Loader\n",
    "\n",
    "[narration]\n",
    "Now the fun part. Loading real files.\n",
    "\n",
    "LangChain has specialized loaders for every file type. Today we'll use two of the most common ones.\n",
    "\n",
    "The PyPDF Loader reads PDF files page by page. Each page becomes a separate document object with the page number in the metadata. This is perfect because when you cite a source, you can say exactly which page the answer came from.\n",
    "\n",
    "The Text Loader handles plain text files. Simple but effective. One file, one document.\n",
    "\n",
    "The beauty of LangChain's design is that after loading, every document looks the same regardless of the original format. PDF, text, web page — they all become document objects with content and metadata. Your entire downstream pipeline stays identical.\n",
    "\n",
    "This is the universal interface pattern. Once loaded, everything looks the same regardless of the original format.\n",
    "\n",
    "[display]\n",
    "## Document Loaders\n",
    "\n",
    "**PyPDFLoader:**\n",
    "```python\n",
    "loader = PyPDFLoader(\"file.pdf\")\n",
    "docs = loader.load()  # One doc per page\n",
    "```\n",
    "\n",
    "**TextLoader:**\n",
    "```python\n",
    "loader = TextLoader(\"file.txt\")\n",
    "docs = loader.load()  # One doc per file\n",
    "```\n",
    "\n",
    "**After loading — everything looks the same:**\n",
    "```\n",
    "Document(\n",
    "    page_content = \"...\",\n",
    "    metadata = {\"source\": \"file.pdf\", \"page\": 0}\n",
    ")\n",
    "```\n",
    "\n",
    "**100+ loaders available:**\n",
    "CSV, Word, HTML, Notion, Slack, YouTube, and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF loaded: 2 pages\n",
      "Tech doc loaded: 1 document(s)\n",
      "Roadmap loaded: 1 document(s)\n",
      "\n",
      "Total documents loaded: 4\n",
      "\n",
      "============================================================\n",
      "\n",
      "Document 1:\n",
      "   Source: company_docs/employee_handbook.pdf\n",
      "   Page: 0\n",
      "   Length: 1475 characters\n",
      "   Preview: NovaTech Employee Handbook 2026\n",
      "Section 1: Leave Policy\n",
      "All full-time employees at NovaTech receive ...\n",
      "\n",
      "Document 2:\n",
      "   Source: company_docs/employee_handbook.pdf\n",
      "   Page: 1\n",
      "   Length: 1371 characters\n",
      "   Preview: Section 3: Compensation & Benefits\n",
      "Salary reviews are conducted annually in April. Performance bonus...\n",
      "\n",
      "Document 3:\n",
      "   Source: company_docs/tech_architecture.txt\n",
      "   Page: N/A\n",
      "   Length: 2732 characters\n",
      "   Preview: NovaTech Technical Architecture Document\n",
      "Last Updated: January 2026\n",
      "\n",
      "1. SYSTEM OVERVIEW\n",
      "NovaTech's p...\n",
      "\n",
      "Document 4:\n",
      "   Source: company_docs/product_roadmap.txt\n",
      "   Page: N/A\n",
      "   Length: 2170 characters\n",
      "   Preview: NovaTech Product Roadmap 2026\n",
      "Status: Approved by Product Council\n",
      "\n",
      "Q1 2026 (January - March): Founda...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"[narration]\n",
    "Let me load all three documents. The PDF loader splits by page, so our two-page handbook becomes two documents. Each text file becomes one document.\n",
    "\n",
    "I combine everything into a single list. Four documents total from three files. Notice the metadata — the PDF documents have both source and page number. The text files have just the source path.\n",
    "\n",
    "I'm printing a preview of each document so you can see exactly what the loaders extracted.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader(\"company_docs/employee_handbook.pdf\")\n",
    "pdf_docs = pdf_loader.load()\n",
    "print(f\"PDF loaded: {len(pdf_docs)} pages\")\n",
    "\n",
    "tech_loader = TextLoader(\"company_docs/tech_architecture.txt\")\n",
    "tech_docs = tech_loader.load()\n",
    "print(f\"Tech doc loaded: {len(tech_docs)} document(s)\")\n",
    "\n",
    "roadmap_loader = TextLoader(\"company_docs/product_roadmap.txt\")\n",
    "roadmap_docs = roadmap_loader.load()\n",
    "print(f\"Roadmap loaded: {len(roadmap_docs)} document(s)\")\n",
    "\n",
    "all_documents = pdf_docs + tech_docs + roadmap_docs\n",
    "print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "for i, doc in enumerate(all_documents):\n",
    "    source = doc.metadata.get('source', 'unknown')\n",
    "    page = doc.metadata.get('page', 'N/A')\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"   Source: {source}\")\n",
    "    print(f\"   Page: {page}\")\n",
    "    print(f\"   Length: {len(doc.page_content)} characters\")\n",
    "    print(f\"   Preview: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[title]\n",
    "Splitting, Embedding, and Storing — The Pipeline You Know\n",
    "\n",
    "[narration]\n",
    "Now we run the same pipeline from Episode 1. Split into chunks, embed them, store in FAISS.\n",
    "\n",
    "But this time, I'm using a slightly larger chunk size — one thousand characters instead of five hundred. Why? Because our documents are longer and more detailed. Larger chunks preserve more context per piece, which helps the language model give more complete answers.\n",
    "\n",
    "There's always a tradeoff. Smaller chunks mean more precise retrieval but less context. Larger chunks mean more context but potentially less precise matches. One thousand with two hundred overlap is a solid default for most production systems.\n",
    "\n",
    "After splitting, we embed and store everything in one line — same as before.\n",
    "\n",
    "[display]\n",
    "## The Pipeline\n",
    "\n",
    "**Chunk size tradeoff:**\n",
    "```\n",
    "Small chunks (500)  --> Precise search, less context\n",
    "Large chunks (1000) --> More context, broader matches\n",
    "```\n",
    "\n",
    "**Our settings:**\n",
    "- Chunk size: 1000 characters\n",
    "- Overlap: 200 characters\n",
    "- Splitter: RecursiveCharacterTextSplitter\n",
    "\n",
    "**Same one-liner from Episode 1:**\n",
    "```python\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original documents: 4\n",
      "After splitting:    11 chunks\n",
      "Avg chunk size:     724 characters\n",
      "\n",
      "✅ Vector store built with 11 chunks from 4 documents\n",
      "\n",
      "Chunks per source:\n",
      "   company_docs/employee_handbook.pdf: 4 chunks\n",
      "   company_docs/tech_architecture.txt: 4 chunks\n",
      "   company_docs/product_roadmap.txt: 3 chunks\n"
     ]
    }
   ],
   "source": [
    "\"\"\"[narration]\n",
    "Split, embed, store. Three lines of code for the entire indexing pipeline.\n",
    "\n",
    "One thousand characters per chunk with two hundred character overlap. The overlap is larger this time because our chunks are larger — we want enough shared context between neighbors.\n",
    "\n",
    "Watch the numbers. We started with four documents. After splitting, we have many more chunks. Each chunk keeps the original metadata, so we never lose track of which file it came from.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(all_documents)\n",
    "print(f\"Original documents: {len(all_documents)}\")\n",
    "print(f\"After splitting:    {len(chunks)} chunks\")\n",
    "print(f\"Avg chunk size:     {sum(len(c.page_content) for c in chunks) // len(chunks)} characters\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "print(f\"\\n✅ Vector store built with {len(chunks)} chunks from {len(all_documents)} documents\")\n",
    "\n",
    "from collections import Counter\n",
    "source_counts = Counter(c.metadata['source'] for c in chunks)\n",
    "print(\"\\nChunks per source:\")\n",
    "for source, count in source_counts.items():\n",
    "    print(f\"   {source}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[title]\n",
    "Similarity Search With Scores — How Confident Is the Match?\n",
    "\n",
    "[narration]\n",
    "Before we build the full RAG chain, let me show you something new — similarity search with scores.\n",
    "\n",
    "In Episode 1, we just got back the matching chunks. But we had no idea how similar they were. Was it a ninety-five percent match or a fifty percent match? That information is crucial.\n",
    "\n",
    "FAISS can return a distance score alongside each result. Lower distance means higher similarity. Think of it like a search engine confidence score — it tells you how relevant each result is.\n",
    "\n",
    "This is powerful for two reasons. First, you can set a threshold. If nothing scores above a certain relevance, you know the answer probably isn't in your documents. Second, you can show users a confidence level, which builds trust.\n",
    "\n",
    "Let me show you the difference.\n",
    "\n",
    "[display]\n",
    "## Similarity Scores\n",
    "\n",
    "**Without scores (Episode 1):**\n",
    "```\n",
    "Results: [doc1, doc2, doc3]\n",
    "\"Are these good matches? Who knows!\"\n",
    "```\n",
    "\n",
    "**With scores (Episode 2):**\n",
    "```\n",
    "Results: [(doc1, 0.35), (doc2, 0.72), (doc3, 1.45)]\n",
    "\"doc1 is highly relevant, doc3 is barely related\"\n",
    "```\n",
    "\n",
    "**FAISS distance:**\n",
    "- Lower = more similar\n",
    "- 0.0 = identical\n",
    "- < 1.0 = good match\n",
    "- > 1.5 = weak match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'How many days of paid leave do I get?'\n",
      "   [company_docs/employee_handbook.pdf] Score: 0.9109\n",
      "      NovaTech Employee Handbook 2026\n",
      "Section 1: Leave Policy\n",
      "All full-time employees at NovaTech receive ...\n",
      "   [company_docs/employee_handbook.pdf] Score: 1.2549\n",
      "      Section 2: Remote Work Policy\n",
      "NovaTech operates a hybrid work model. Employees may work remotely up ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Query: 'What database does the platform use?'\n",
      "   [company_docs/tech_architecture.txt] Score: 0.9583\n",
      "      3. DATABASE LAYER\n",
      "Primary database is PostgreSQL 16 running on AWS RDS with read replicas in two ava...\n",
      "   [company_docs/product_roadmap.txt] Score: 1.3597\n",
      "      Q2 2026 (April - June): Growth Phase\n",
      "- Team collaboration features: Real-time document editing, in-a...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Query: 'What is the recipe for chocolate cake?'\n",
      "   [company_docs/product_roadmap.txt] Score: 1.9450\n",
      "      NovaTech Product Roadmap 2026\n",
      "Status: Approved by Product Council\n",
      "\n",
      "Q1 2026 (January - March): Founda...\n",
      "   [company_docs/employee_handbook.pdf] Score: 1.9553\n",
      "      75000 for courses, certifications, conferences, and books. Unused learning budget does not carry ove...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Lower score = better match\n",
      "The chocolate cake query has HIGH scores — system knows it's irrelevant!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"[narration]\n",
    "The method is called similarity search with score. Instead of just returning documents, it returns tuples — each containing the document and its distance score.\n",
    "\n",
    "I'll test three questions. One about leave policy, one about deployment, and one that's completely outside our knowledge base. Watch how the scores differ dramatically.\n",
    "\n",
    "The relevant questions should have low distance scores — meaning high similarity. The irrelevant question should have high scores — meaning the system knows it doesn't have a good match.\n",
    "\"\"\"\n",
    "\n",
    "test_queries = [\n",
    "    \"How many days of paid leave do I get?\",\n",
    "    \"What database does the platform use?\",\n",
    "    \"What is the recipe for chocolate cake?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    results_with_scores = vectorstore.similarity_search_with_score(query, k=2)\n",
    "\n",
    "    for doc, score in results_with_scores:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        print(f\"   [{source}] Score: {score:.4f}\")\n",
    "        print(f\"      {doc.page_content[:100]}...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print()\n",
    "print(\"Lower score = better match\")\n",
    "print(\"The chocolate cake query has HIGH scores — system knows it's irrelevant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[title]\n",
    "Building the Basic RAG Chain — Quick Refresher\n",
    "\n",
    "[narration]\n",
    "Let me quickly build the standard RAG chain — same pattern as Episode 1 — so we have a baseline to compare with the conversational version.\n",
    "\n",
    "Retriever fetches the top three chunks. Prompt tells the language model to answer only from context. Parser extracts clean text.\n",
    "\n",
    "Quick test to make sure everything works before we add memory.\n",
    "\n",
    "[display]\n",
    "## Basic RAG Chain (Refresher)\n",
    "\n",
    "```\n",
    "Question --> Retriever --> Prompt --> LLM --> Answer\n",
    "```\n",
    "\n",
    "**Same pattern as Episode 1 — just with real documents now!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the home office setup allowance?\n",
      "A: The home office setup allowance is Rs 25000, provided as a one-time allowance for all eligible employees. [Source: company_docs/employee_handbook.pdf, Page 1]\n",
      "------------------------------------------------------------\n",
      "Q: How is the CI/CD pipeline structured?\n",
      "A: The CI/CD pipeline is structured with the following stages: lint, unit test, integration test, security scan, build, deploy to staging, automated E2E tests, and production deploy with canary rollout. [Source: company_docs/tech_architecture.txt, section 5. DEPLOYMENT & INFRASTRUCTURE]\n",
      "------------------------------------------------------------\n",
      "Q: What features are planned for Q2 2026?\n",
      "A: For Q2 2026, the following features are planned: \n",
      "1. Team collaboration features, \n",
      "2. Enterprise SSO, and \n",
      "3. Advanced analytics. [Source: company_docs/product_roadmap.txt]\n",
      "------------------------------------------------------------\n",
      "\n",
      "✅ Basic RAG works with real documents!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"[narration]\n",
    "Standard RAG chain. You've seen this before. Retriever with k equals three, anti-hallucination prompt, and the chain connected with pipe operators.\n",
    "\n",
    "Let me test with a few questions across different documents.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted = []\n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        page = doc.metadata.get('page', '')\n",
    "        page_str = f\", Page {page + 1}\" if isinstance(page, int) else \"\"\n",
    "        formatted.append(f\"[Source: {source}{page_str}]\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful company assistant for NovaTech. Answer questions using ONLY the provided context.\n",
    "\n",
    "Rules:\n",
    "- Answer based ONLY on the context below\n",
    "- If the answer is not in the context, say: \"I don't have that information in my knowledge base.\"\n",
    "- Always cite the source document and page when available\n",
    "- Be concise and specific\"\"\"),\n",
    "    (\"human\", \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")\n",
    "])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "questions = [\n",
    "    \"What is the home office setup allowance?\",\n",
    "    \"How is the CI/CD pipeline structured?\",\n",
    "    \"What features are planned for Q2 2026?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {rag_chain.invoke(q)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print()\n",
    "print(\"✅ Basic RAG works with real documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[title]\n",
    "The Problem — Why Basic RAG Can't Handle Conversations\n",
    "\n",
    "[narration]\n",
    "Now watch this. I'm going to expose the fatal flaw in our basic RAG chain.\n",
    "\n",
    "First question: 'What is the learning budget at NovaTech?' It answers perfectly. Seventy-five thousand rupees per year.\n",
    "\n",
    "Second question: 'Does it carry over to the next year?'\n",
    "\n",
    "And here's the problem. The word 'it' refers to the learning budget. You and I understand that from context. But the RAG system doesn't. Each question is processed independently. There's no memory. No context from the previous question.\n",
    "\n",
    "So when the retriever searches for 'Does it carry over?', it doesn't know what 'it' refers to. It might find documents about leave carry-over instead. Or it might find nothing relevant at all.\n",
    "\n",
    "This is the classic follow-up question problem. And it's why every real chatbot needs conversational RAG.\n",
    "\n",
    "The solution? We need to rewrite the follow-up question into a standalone question before searching. 'Does it carry over?' becomes 'Does the learning budget carry over to the next year?' Now the retriever knows exactly what to look for.\n",
    "\n",
    "[display]\n",
    "## The Follow-Up Problem\n",
    "\n",
    "**Conversation:**\n",
    "```\n",
    "User: \"What is the learning budget?\"\n",
    "AI:   \"Rs 75,000 per year.\"\n",
    "User: \"Does it carry over?\"\n",
    "AI:   ??? (what is 'it'?)\n",
    "```\n",
    "\n",
    "**The fix — rewrite before searching:**\n",
    "```\n",
    "\"Does it carry over?\"\n",
    "      | (rewrite using chat history)\n",
    "\"Does the learning budget carry over?\"\n",
    "      | (now search works!)\n",
    "```\n",
    "\n",
    "**This is called: History-Aware Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASIC RAG — No Memory:\n",
      "\n",
      "Q1: What is the learning budget at NovaTech?\n",
      "A1: The annual learning budget at NovaTech is Rs 75000 for courses, certifications, conferences, and books. \n",
      "[Source: company_docs/employee_handbook.pdf, Page 2 and Page 2, Section 4: Professional Development]\n",
      "\n",
      "Q2: Does it carry over to the next year?\n",
      "  --> Retriever found these sources (without context):\n",
      "      company_docs/employee_handbook.pdf, Page 2: 75000 for courses, certifications, conferences, and books. Unused learning budge...\n",
      "      company_docs/employee_handbook.pdf, Page 2: Section 3: Compensation & Benefits\n",
      "Salary reviews are conducted annually in Apri...\n",
      "      company_docs/product_roadmap.txt: Q3 2026 (July - September): Scale Phase\n",
      "- Multi-region deployment: Expand infras...\n",
      "A2: No, the unused learning budget does not carry over to the next year. [Source: company_docs/employee_handbook.pdf, Page 2]\n",
      "\n",
      "Notice: 'it' is ambiguous without history!\n",
      "The retriever may find leave carry-over instead of learning budget carry-over.\n",
      "That's why we need conversational RAG — to rewrite the question first.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"[narration]\n",
    "Let me prove the problem first. I'll ask about the learning budget, then ask a follow-up.\n",
    "\n",
    "But this time, I'm also printing what the retriever finds for the follow-up question. This debug output shows you exactly why it fails — the retriever has no idea what 'it' means, so it searches blindly and may pull the wrong documents entirely.\n",
    "\"\"\"\n",
    "\n",
    "print(\"BASIC RAG — No Memory:\")\n",
    "print()\n",
    "\n",
    "q1 = \"What is the learning budget at NovaTech?\"\n",
    "a1 = rag_chain.invoke(q1)\n",
    "print(f\"Q1: {q1}\")\n",
    "print(f\"A1: {a1}\")\n",
    "print()\n",
    "\n",
    "q2 = \"Does it carry over to the next year?\"\n",
    "\n",
    "# Debug: show what the retriever actually finds for the vague follow-up\n",
    "retrieved = retriever.invoke(q2)\n",
    "print(f\"Q2: {q2}\")\n",
    "print(f\"  --> Retriever found these sources (without context):\")\n",
    "for d in retrieved:\n",
    "    src = d.metadata.get('source', 'unknown')\n",
    "    page = d.metadata.get('page', '')\n",
    "    page_str = f\", Page {page + 1}\" if isinstance(page, int) else \"\"\n",
    "    print(f\"      {src}{page_str}: {d.page_content[:80]}...\")\n",
    "\n",
    "a2 = rag_chain.invoke(q2)\n",
    "print(f\"A2: {a2}\")\n",
    "\n",
    "print()\n",
    "print(\"Notice: 'it' is ambiguous without history!\")\n",
    "print(\"The retriever may find leave carry-over instead of learning budget carry-over.\")\n",
    "print(\"That's why we need conversational RAG — to rewrite the question first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[title]\n",
    "Conversational RAG — Adding Memory That Actually Works\n",
    "\n",
    "[narration]\n",
    "Here's the architecture for conversational RAG. It has two stages.\n",
    "\n",
    "Stage one — Question Rewriting. Before we search, we pass the chat history AND the new question to the language model. We ask it: given this conversation history, rewrite the latest question so it makes sense as a standalone question. 'Does it carry over?' becomes 'Does the NovaTech learning budget carry over to the next year?'\n",
    "\n",
    "Stage two — Standard RAG. Now we take the rewritten question and run our normal RAG pipeline. Search, retrieve, generate.\n",
    "\n",
    "The key insight is that we use the language model twice. Once to rewrite the question. Once to generate the answer. Two calls, but the result is a chatbot that actually understands follow-ups.\n",
    "\n",
    "This is exactly how production chatbots handle multi-turn conversations. The pattern is called history-aware retrieval.\n",
    "\n",
    "[display]\n",
    "## Conversational RAG Architecture\n",
    "\n",
    "**Two-stage pipeline:**\n",
    "```\n",
    "Stage 1: REWRITE\n",
    "  Chat History + New Question\n",
    "      --> LLM rewrites into standalone question\n",
    "\n",
    "Stage 2: RAG (same as before)\n",
    "  Standalone Question\n",
    "      --> Retriever --> Prompt --> LLM --> Answer\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "History: \"What is the learning budget?\" --> \"Rs 75,000\"\n",
    "New:     \"Does it carry over?\"\n",
    "Rewrite: \"Does the NovaTech learning budget carry over?\"\n",
    "```\n",
    "\n",
    "**Uses LLM twice:** once to rewrite, once to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Rewriting Demo:\n",
      "\n",
      "History: 'Human: What is the learning budget at NovaTech?\n",
      "AI: Each employee receives an annual learning budget of Rs 75,000 for courses, certifications, conferences, and books.'\n",
      "\n",
      "   Original:  'Does it carry over to the next year?'\n",
      "   Rewritten: 'Does the annual learning budget of Rs 75,000 provided to each employee at NovaTech carry over to the next year if it is not fully utilized?'\n",
      "\n",
      "   Original:  'What about conferences?'\n",
      "   Rewritten: 'What is the policy regarding the use of the annual learning budget of Rs 75,000 for attending conferences at NovaTech?'\n",
      "\n",
      "   Original:  'How does this compare to the home office allowance?'\n",
      "   Rewritten: 'What is the comparison between the annual learning budget of Rs 75,000 for courses, certifications, conferences, and books at NovaTech and the home office allowance?'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"[narration]\n",
    "First, the question rewriting chain. This is the magic ingredient.\n",
    "\n",
    "I define a prompt that gives the language model the chat history and the latest question. The instruction is clear: rewrite the question so it stands on its own. If the question already makes sense without context, return it unchanged.\n",
    "\n",
    "Let me test this with our problematic follow-up question. Watch how it transforms 'Does it carry over?' into a complete, searchable question.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Given the following conversation history and a follow-up question,\n",
    "rewrite the follow-up question to be a standalone question that includes all necessary context.\n",
    "If the question already makes sense on its own, return it unchanged.\n",
    "Only return the rewritten question — nothing else.\"\"\"),\n",
    "    (\"human\", \"\"\"Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Follow-up Question: {question}\n",
    "\n",
    "Standalone Question:\"\"\")\n",
    "])\n",
    "\n",
    "rewrite_chain = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "sample_history = \"\"\"Human: What is the learning budget at NovaTech?\n",
    "AI: Each employee receives an annual learning budget of Rs 75,000 for courses, certifications, conferences, and books.\"\"\"\n",
    "\n",
    "follow_ups = [\n",
    "    \"Does it carry over to the next year?\",\n",
    "    \"What about conferences?\",\n",
    "    \"How does this compare to the home office allowance?\",\n",
    "]\n",
    "\n",
    "print(\"Question Rewriting Demo:\")\n",
    "print()\n",
    "print(f\"History: '{sample_history}'\")\n",
    "print()\n",
    "\n",
    "for q in follow_ups:\n",
    "    rewritten = rewrite_chain.invoke({\n",
    "        \"chat_history\": sample_history,\n",
    "        \"question\": q\n",
    "    })\n",
    "    print(f\"   Original:  '{q}'\")\n",
    "    print(f\"   Rewritten: '{rewritten}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conversational RAG system ready!\n",
      "   Memory: enabled\n",
      "   Question rewriting: enabled\n"
     ]
    }
   ],
   "source": [
    "\"\"\"[narration]\n",
    "Now the complete conversational RAG system. I wrap everything into a class to keep the state clean.\n",
    "\n",
    "The class stores the chat history as a list of messages. When you ask a question, it first checks if there's any history. If there is, it rewrites the question. If not, it uses the question as is.\n",
    "\n",
    "Then it runs the standard RAG pipeline with the rewritten question. After getting the answer, it saves both the question and answer to history.\n",
    "\n",
    "One important note. This is a simple memory approach — we store the full conversation as-is. In production, you'd summarize or window the chat history to prevent it from growing too large. Long histories increase cost, add latency, and can actually degrade the rewriting quality. But for learning, this pattern shows you the core idea clearly.\n",
    "\"\"\"\n",
    "\n",
    "class ConversationalRAG:\n",
    "\n",
    "    def __init__(self, retriever, llm, embeddings):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.chat_history = []\n",
    "\n",
    "        self.rewrite_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Given the conversation history and a follow-up question,\n",
    "rewrite the question to be standalone with all necessary context.\n",
    "If it already stands alone, return it unchanged. Only return the question.\"\"\"),\n",
    "            (\"human\", \"\"\"Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Follow-up Question: {question}\n",
    "\n",
    "Standalone Question:\"\"\")\n",
    "        ])\n",
    "        self.rewrite_chain = self.rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "        self.rag_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a helpful NovaTech company assistant. Answer using ONLY the provided context.\n",
    "- If the answer is not in the context, say so\n",
    "- Cite the source document and page when available\n",
    "- Be concise and specific\"\"\"),\n",
    "            (\"human\", \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")\n",
    "        ])\n",
    "\n",
    "    def _format_history(self):\n",
    "        if not self.chat_history:\n",
    "            return \"(No previous conversation)\"\n",
    "        lines = []\n",
    "        for msg in self.chat_history:\n",
    "            role = \"Human\" if msg[\"role\"] == \"human\" else \"AI\"\n",
    "            lines.append(f\"{role}: {msg['content']}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def ask(self, question):\n",
    "        if self.chat_history:\n",
    "            standalone_q = self.rewrite_chain.invoke({\n",
    "                \"chat_history\": self._format_history(),\n",
    "                \"question\": question\n",
    "            })\n",
    "        else:\n",
    "            standalone_q = question\n",
    "\n",
    "        docs = self.retriever.invoke(standalone_q)\n",
    "        context = format_docs(docs)\n",
    "        messages = self.rag_prompt.format_messages(context=context, question=standalone_q)\n",
    "        answer = self.llm.invoke(messages).content\n",
    "\n",
    "        self.chat_history.append({\"role\": \"human\", \"content\": question})\n",
    "        self.chat_history.append({\"role\": \"ai\", \"content\": answer})\n",
    "\n",
    "        sources = []\n",
    "        for d in docs:\n",
    "            src = d.metadata.get('source', 'unknown')\n",
    "            page = d.metadata.get('page', None)\n",
    "            if isinstance(page, int):\n",
    "                sources.append(f\"{src} (Page {page + 1})\")\n",
    "            else:\n",
    "                sources.append(src)\n",
    "        sources = sorted(set(sources))\n",
    "        return answer, sources, standalone_q\n",
    "\n",
    "    def reset(self):\n",
    "        self.chat_history = []\n",
    "\n",
    "conv_rag = ConversationalRAG(retriever, llm, embeddings)\n",
    "print(\"✅ Conversational RAG system ready!\")\n",
    "print(\"   Memory: enabled\")\n",
    "print(\"   Question rewriting: enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[title]\n",
    "The Moment of Truth — Follow-Up Questions That Actually Work\n",
    "\n",
    "[narration]\n",
    "Let's test this with a multi-turn conversation. I'll start with a question, then ask follow-ups that use pronouns and references to previous answers. The exact scenario that broke our basic RAG chain.\n",
    "\n",
    "Watch two things. First, the rewritten questions — see how the system transforms vague follow-ups into precise standalone questions. Second, the answers — they should be accurate and relevant, using the right context every time.\n",
    "\n",
    "[display]\n",
    "## Testing Conversational RAG\n",
    "\n",
    "**Our test conversation:**\n",
    "1. Start with a topic\n",
    "2. Ask follow-up with \"it\" / \"that\"\n",
    "3. Compare with a different topic\n",
    "4. Reference something from earlier\n",
    "\n",
    "**Watching for:**\n",
    "- ✅ Correct question rewriting\n",
    "- ✅ Accurate answers from right documents\n",
    "- ✅ Memory across turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERSATIONAL RAG — With Memory:\n",
      "\n",
      "============================================================\n",
      "\n",
      "Turn 1: What is the learning budget at NovaTech?\n",
      "Answer: The annual learning budget at NovaTech is Rs 75000 for courses, certifications, conferences, and books. \n",
      "[Source: company_docs/employee_handbook.pdf, Page 2]\n",
      "Sources: company_docs/employee_handbook.pdf (Page 1), company_docs/employee_handbook.pdf (Page 2)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Turn 2: Does it carry over to the next year?\n",
      "Rewritten: Does the annual learning budget of Rs 75000 at NovaTech carry over to the next year if it is not fully utilized?\n",
      "Answer: No, the annual learning budget of Rs 75000 at NovaTech does not carry over to the next year if it is not fully utilized. \n",
      "[Source: company_docs/employee_handbook.pdf, Page 2]\n",
      "Sources: company_docs/employee_handbook.pdf (Page 1), company_docs/employee_handbook.pdf (Page 2)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Turn 3: What about the home office allowance — how much is that?\n",
      "Rewritten: What is the home office allowance at NovaTech?\n",
      "Answer: The home office allowance at NovaTech is a one-time payment of Rs 25,000 for all eligible employees. [Source: company_docs/employee_handbook.pdf, Page 1]\n",
      "Sources: company_docs/employee_handbook.pdf (Page 1), company_docs/employee_handbook.pdf (Page 2)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Turn 4: Which of those two benefits is more generous?\n",
      "Rewritten: Which of the annual learning budget of Rs 75000 and the one-time home office allowance of Rs 25,000 at NovaTech is more generous?\n",
      "Answer: The annual learning budget of Rs 75000 is more generous than the one-time home office allowance of Rs 25,000. \n",
      "\n",
      "(Source: company_docs/employee_handbook.pdf, Pages 1 and 2)\n",
      "Sources: company_docs/employee_handbook.pdf (Page 1), company_docs/employee_handbook.pdf (Page 2)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Turn 5: Tell me about the deployment schedule for the engineering team.\n",
      "Rewritten: What is the deployment schedule for the engineering team at NovaTech?\n",
      "Answer: The provided context does not mention the deployment schedule for the engineering team at NovaTech.\n",
      "Sources: company_docs/employee_handbook.pdf (Page 1), company_docs/tech_architecture.txt\n",
      "------------------------------------------------------------\n",
      "\n",
      "Conversation history: 10 messages\n",
      "✅ Follow-up questions work perfectly!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"[narration]\n",
    "Five turns of conversation. Starting with the learning budget, then asking follow-ups that reference previous answers.\n",
    "\n",
    "For each turn, I'm showing the original question, the rewritten version, the answer, and the sources. This makes it crystal clear how the rewriting works.\n",
    "\"\"\"\n",
    "\n",
    "conversation = [\n",
    "    \"What is the learning budget at NovaTech?\",\n",
    "    \"Does it carry over to the next year?\",\n",
    "    \"What about the home office allowance — how much is that?\",\n",
    "    \"Which of those two benefits is more generous?\",\n",
    "    \"Tell me about the deployment schedule for the engineering team.\",\n",
    "]\n",
    "\n",
    "print(\"CONVERSATIONAL RAG — With Memory:\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, q in enumerate(conversation, 1):\n",
    "    answer, sources, rewritten = conv_rag.ask(q)\n",
    "    print(f\"\\nTurn {i}: {q}\")\n",
    "    if rewritten != q:\n",
    "        print(f\"Rewritten: {rewritten}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"Sources: {', '.join(sources)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nConversation history: {len(conv_rag.chat_history)} messages\")\n",
    "print(\"✅ Follow-up questions work perfectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[title]\n",
    "Interactive Conversational RAG Demo\n",
    "\n",
    "[narration]\n",
    "Let me put this all together in an interactive demo. This time, the chatbot remembers everything you say. You can ask follow-ups, reference previous answers, switch topics, and come back to earlier questions.\n",
    "\n",
    "Try it yourself. Start with any topic from the handbook, tech docs, or product roadmap. Then ask follow-ups. Test the memory.\n",
    "\n",
    "Type 'reset' to clear the conversation and start fresh. Type 'quit' to exit.\n",
    "\n",
    "[display]\n",
    "## Interactive Demo — Conversational RAG\n",
    "\n",
    "**Try these conversation flows:**\n",
    "\n",
    "Flow 1 — HR Deep Dive:\n",
    "- \"What is the remote work policy?\"\n",
    "- \"Which days do I need to come in?\"\n",
    "- \"What if I'm a new employee?\"\n",
    "\n",
    "Flow 2 — Tech Exploration:\n",
    "- \"What's the tech stack?\"\n",
    "- \"How is data cached?\"\n",
    "- \"What about security?\"\n",
    "\n",
    "Flow 3 — Product Planning:\n",
    "- \"What's coming in Q2?\"\n",
    "- \"How much will that cost?\"\n",
    "- \"Is there anything related to AI?\"\n",
    "\n",
    "**Commands:** `reset` = clear memory, `quit` = exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting interactive conversational RAG...\n",
      "\n",
      "============================================================\n",
      "NOVATECH KNOWLEDGE BASE — Conversational RAG\n",
      "============================================================\n",
      "\n",
      "Ask anything! I remember the conversation.\n",
      "Commands: 'reset' = clear memory, 'quit' = exit\n",
      "\n",
      "Answer: NovaTech is a company that offers a microservices-based SaaS application. [Source: company_docs/tech_architecture.txt]\n",
      "\n",
      "Sources: company_docs/employee_handbook.pdf (Page 1), company_docs/employee_handbook.pdf (Page 2), company_docs/tech_architecture.txt\n",
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"[narration]\n",
    "Interactive conversational RAG. Every answer builds on the previous ones. The reset command clears memory so you can start a new topic cleanly.\n",
    "\n",
    "I'll demonstrate a quick conversation, then hand it over to you.\n",
    "\"\"\"\n",
    "\n",
    "def interactive_conversational_rag():\n",
    "    rag = ConversationalRAG(retriever, llm, embeddings)\n",
    "\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"NOVATECH KNOWLEDGE BASE — Conversational RAG\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"Ask anything! I remember the conversation.\")\n",
    "    print(\"Commands: 'reset' = clear memory, 'quit' = exit\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "\n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "\n",
    "        if user_input.lower() == 'reset':\n",
    "            rag.reset()\n",
    "            print(\"Memory cleared! Starting fresh.\")\n",
    "            continue\n",
    "\n",
    "        if not user_input:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            answer, sources, rewritten = rag.ask(user_input)\n",
    "            if rewritten != user_input:\n",
    "                print(f\"   Understood as: {rewritten}\")\n",
    "            print(f\"\\nAnswer: {answer}\")\n",
    "            print(f\"\\nSources: {', '.join(sources)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "            break\n",
    "\n",
    "print(\"Starting interactive conversational RAG...\")\n",
    "interactive_conversational_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[title]\n",
    "What You Just Built — And What's Coming Next\n",
    "\n",
    "[narration]\n",
    "Let's recap what you accomplished today.\n",
    "\n",
    "You loaded real PDF and text files using LangChain's document loaders. You saw how different file formats become identical document objects after loading.\n",
    "\n",
    "You learned about similarity scores and how they tell you how confident the retrieval is. Lower score, better match.\n",
    "\n",
    "And the big one — you built conversational RAG. A system that rewrites follow-up questions using the chat history before searching. This is the exact pattern used in production chatbots at companies handling millions of conversations.\n",
    "\n",
    "The two-stage architecture — rewrite then retrieve — is elegant and powerful. And now you understand every piece of it.\n",
    "\n",
    "In Episode 3, we're going even deeper. Advanced retrieval strategies. Multiple retrieval methods combined. Re-ranking results for maximum accuracy. And metadata filtering so you can search within specific documents or categories.\n",
    "\n",
    "But right now, while this is fresh, run this notebook. Load your own PDFs. Have a multi-turn conversation with your own documents. The learning happens when you build.\n",
    "\n",
    "Subscribe to NoteBook Learnings. Drop a comment with what you built.\n",
    "\n",
    "I'll see you in the next one.\n",
    "\n",
    "[display]\n",
    "## What You Built Today\n",
    "\n",
    "**New skills unlocked:**\n",
    "1. Loading real PDFs and text files\n",
    "2. Similarity search with confidence scores\n",
    "3. Conversational RAG with memory\n",
    "4. History-aware question rewriting\n",
    "5. Multi-turn follow-up conversations\n",
    "\n",
    "**The conversational RAG pattern:**\n",
    "```python\n",
    "# Stage 1: Rewrite\n",
    "standalone = rewrite_chain.invoke({\n",
    "    \"chat_history\": history,\n",
    "    \"question\": follow_up\n",
    "})\n",
    "\n",
    "# Stage 2: Standard RAG\n",
    "answer = rag_chain.invoke(standalone)\n",
    "```\n",
    "\n",
    "**RAG Series — NoteBook Learnings:**\n",
    "- Ep 1: RAG From Scratch  ✅\n",
    "- Ep 2: Real Docs + Conversational RAG  <-- YOU ARE HERE\n",
    "- Ep 3: Advanced Retrieval Strategies\n",
    "- Ep 4: Multi-Modal RAG (images + tables)\n",
    "- Ep 5: Production RAG Deployment\n",
    "\n",
    "**Your homework:**\n",
    "1. Load your own PDF into this pipeline\n",
    "2. Have a 5-turn conversation with your document\n",
    "3. Try different chunk sizes and compare answers\n",
    "4. Test with questions outside the document scope\n",
    "5. Share your results in the comments!\n",
    "\n",
    "**Subscribe** to NoteBook Learnings\n",
    "**Download** the notebook from the description"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_videos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
